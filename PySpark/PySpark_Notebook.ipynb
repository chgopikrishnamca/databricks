{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f93ab8b-da06-49bd-b6cc-dbfd1d0c9e8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Youtube links: https://www.youtube.com/watch?v=8nNTG5fH9do&list=PLNRxk1s77zfj2M2MuCuEPy_k25bUbv1Py&index=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9595f9e1-00de-4738-b503-d5337ad8121e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Self Join"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input:\n",
    "+----+\n",
    "|team|\n",
    "+----+\n",
    "| PAK|\n",
    "| AFG|\n",
    "| IND|\n",
    "| AUS|\n",
    "+----+\n",
    "O/p:\n",
    "+-----------------+\n",
    "|Match Combination|\n",
    "+-----------------+\n",
    "|AFG Vs PAK       |\n",
    "|AFG Vs IND       |\n",
    "|AFG Vs AUS       |\n",
    "|IND Vs PAK       |\n",
    "|AUS Vs PAK       |\n",
    "|AUS Vs IND       |\n",
    "+-----------------+\n",
    "\n",
    "select concat(t1.teams, \" Vs \", t2.teams) as \"Match Combination\" from teams t1 join teams t2 on t1.teams<t2.teams\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import col, concat_ws\n",
    "# Sample team list\n",
    "teams = [(\"PAK\",), (\"AFG\",), (\"IND\",), (\"AUS\",)]\n",
    "df = spark.createDataFrame(teams, [\"team\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "# Perform self-join\n",
    "joined_df = df.alias(\"df1\").join(df.alias(\"df2\"),\n",
    "                                 col(\"df1.team\") < col(\"df2.team\")) \n",
    "\n",
    "# Combine into match string\n",
    "result_df = joined_df.select(concat_ws(\" Vs \", col(\"df1.team\"), col(\"df2.team\")).alias(\"Match Combination\"))\n",
    "\n",
    "# Display result\n",
    "result_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7160f8bc-1369-4610-9fe5-ba6a00d8bd81",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Joins & Aggregations"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input Table1:\n",
    "+---+-----+\n",
    "| Id| Name|\n",
    "+---+-----+\n",
    "|  1|Steve|\n",
    "|  2|David|\n",
    "|  3| John|\n",
    "|  4|Shree|\n",
    "|  5|Helen|\n",
    "+---+-----+\n",
    "Input Table2:\n",
    "+-------+-----+---+\n",
    "|Subject|Marks| Id|\n",
    "+-------+-----+---+\n",
    "|    SQL|   90|  1|\n",
    "|PySpark|  100|  1|\n",
    "|    SQL|   70|  2|\n",
    "|PySpark|   60|  2|\n",
    "|    SQL|   30|  3|\n",
    "|PySpark|   20|  3|\n",
    "|    SQL|   50|  4|\n",
    "|PySpark|   50|  4|\n",
    "|    SQL|   45|  5|\n",
    "|PySpark|   45|  5|\n",
    "+-------+-----+---+\n",
    "\n",
    "Output:\n",
    "+---+-----+----------+------------+\n",
    "| Id| Name|Percentage|      Result|\n",
    "+---+-----+----------+------------+\n",
    "|  1|Steve|      95.0| Distinction|\n",
    "|  2|David|      65.0|Second Class|\n",
    "|  3| John|      25.0|        Fail|\n",
    "|  4|Shree|      50.0|Second Class|\n",
    "|  5|Helen|      45.0|        Fail|\n",
    "+---+-----+----------+------------+\n",
    "SQL:\n",
    "SELECT \n",
    "    s.Id,\n",
    "    s.Name,\n",
    "    ROUND(AVG(m.Marks), 1) AS Percentage,\n",
    "    CASE \n",
    "        WHEN AVG(m.Marks) >= 75 THEN 'Distinction'\n",
    "        WHEN AVG(m.Marks) >= 50 THEN 'Second Class'\n",
    "        ELSE 'Fail'\n",
    "    END AS Result\n",
    "FROM \n",
    "    Table1 s\n",
    "JOIN \n",
    "    Table2 m ON s.Id = m.Id\n",
    "GROUP BY \n",
    "    s.Id, s.Name;\n",
    "\"\"\"\n",
    "from pyspark.sql.functions import col, sum, when, avg\n",
    "\n",
    "data1 = [(1, \"Steve\"), (2, \"David\"), (3, \"John\"), (4, \"Shree\"), (5, \"Helen\")]\n",
    "data2 = [(\"SQL\", 90, 1), (\"PySpark\", 100, 1), (\"SQL\", 70, 2), (\"PySpark\", 60, 2),\n",
    "         (\"SQL\", 30, 3), (\"PySpark\", 20, 3), (\"SQL\", 50, 4), (\"PySpark\", 50, 4),\n",
    "         (\"SQL\", 45, 5), (\"PySpark\", 45, 5)]\n",
    "\n",
    "schema1 = [\"Id\", \"Name\"]\n",
    "schema2 = [\"Subject\", \"Marks\", \"Id\"]\n",
    "\n",
    "\n",
    "df1 = spark.createDataFrame(data1, schema1)\n",
    "df2 = spark.createDataFrame(data2, schema2)\n",
    "\n",
    "df1.show()\n",
    "df2.show()\n",
    "\n",
    "\n",
    "\n",
    "join_marks_df = df1.join(df2, \"Id\")\n",
    "percentage_df = join_marks_df.groupBy(\"Id\", \"Name\").agg((avg(col(\"Marks\"))).alias(\"Percentage\"))\n",
    "percentage_df.show()\n",
    "\n",
    "final_df = percentage_df.select(\"Id\", \"Name\", \"Percentage\")\\\n",
    "                        .withColumn(\"Result\", \n",
    "                                    when(col(\"Percentage\")>70, \"Distinction\")\n",
    "                                    .when(col(\"Percentage\")>=50, \"Second Class\")\n",
    "                                    .when(col(\"Percentage\")<50, \"Fail\"))\n",
    "final_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bae94508-b179-4ce2-949f-c29939b13d3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Diplay percentage of each product contribution in the store.\n",
    "Input:\n",
    "-------------------------------------\n",
    "Store       |   Product    |    Sales\n",
    "-------------------------------------\n",
    "S1          |   P1         |    100\n",
    "S1          |   P2         |    200\n",
    "S2          |   P1         |    300\n",
    "S2          |   P2         |    400\n",
    "S3          |   P1         |    500\n",
    "S3          |   P2         |    600\n",
    "--------------------------------------\n",
    "\n",
    "Output:\n",
    "+-----+-------+-----+---------------------+---------------+\n",
    "|Store|Product|Sales|Total Sales Per Store|Product Sales %|\n",
    "+-----+-------+-----+---------------------+---------------+\n",
    "|   S1|     P1|  100|                  300|          33.33|\n",
    "|   S1|     P2|  200|                  300|          66.67|\n",
    "|   S2|     P1|  300|                  700|          42.86|\n",
    "|   S2|     P2|  400|                  700|          57.14|\n",
    "|   S3|     P1|  500|                 1100|          45.45|\n",
    "|   S3|     P2|  600|                 1100|          54.55|\n",
    "+-----+-------+-----+---------------------+---------------+\n",
    "\"\"\"\n",
    "from pyspark.sql.functions import col, sum, round\n",
    "\n",
    "data = [(\"S1\",\"P1\",100),(\"S1\",\"P2\",200),(\"S2\",\"P1\",300),(\"S2\",\"P2\",400),(\"S3\",\"P1\",500),(\"S3\",\"P2\",600)]\n",
    "colums = [\"Store\",\"Product\",\"Sales\"]\n",
    "\n",
    "df1 = spark.createDataFrame(data, colums)\n",
    "\n",
    "#Total sales per store\n",
    "df_total_sales_per_store = df1.groupBy(\"Store\").agg(sum(\"Sales\").alias(\"Total Sales Per Store\"))\n",
    "\n",
    "#Join total sales to the original df\n",
    "joined_df = df1.join(df_total_sales_per_store, \"Store\")\n",
    "\n",
    "#caliculating contribution\n",
    "df_product_sales = joined_df.withColumn(\"Product Sales %\", round(((col(\"Sales\")/col(\"Total Sales Per Store\"))*100),2))\n",
    "\n",
    "df_product_sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79711bfb-2926-49bf-b3bb-dc114e208e31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Listing out month. year from the given input date\n",
    "+--------+----------+------+\n",
    "|order_id|order_date| price|\n",
    "+--------+----------+------+\n",
    "|    1001|2025-06-03|150.75|\n",
    "|    1002|2025-06-14| 89.99|\n",
    "|    1003|2025-06-25|200.00|\n",
    "|    1004|2025-07-01|175.20|\n",
    "|    1005|2025-07-11| 49.95|\n",
    "|    1006|2025-07-20|300.50|\n",
    "|    1007|2025-08-02|120.00|\n",
    "|    1008|2025-08-05|245.75|\n",
    "|    1009|2025-08-12|180.40|\n",
    "|    1010|2025-08-18| 95.99|\n",
    "+--------+----------+------+\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql.functions import month, year, date_format\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (1001, datetime(2025, 6, 3), 150.75),\n",
    "    (1002, datetime(2025, 6, 14), 89.99),\n",
    "    (1003, datetime(2025, 6, 25), 200.00),\n",
    "    (1004, datetime(2025, 7, 1), 175.20),\n",
    "    (1005, datetime(2025, 7, 11), 49.95),\n",
    "    (1006, datetime(2025, 7, 20), 300.50),\n",
    "    (1007, datetime(2025, 8, 2), 120.00),\n",
    "    (1008, datetime(2025, 8, 5), 245.75),\n",
    "    (1009, datetime(2025, 8, 12), 180.40),\n",
    "    (1010, datetime(2025, 8, 18), 95.99)\n",
    "]\n",
    "\n",
    "colums = [\"order_id\", \"order_date\", \"price\"]\n",
    "\n",
    "df = spark.createDataFrame(data, colums)\n",
    "\n",
    "#fetch only month and year separately in another colums\n",
    "df = df.withColumn(\"month\", date_format(\"order_date\",\"MMMM\" )).withColumn(\"year\", year(\"order_date\"))\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd13fffb-723a-4e5c-835f-493c4482ee7b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "explode"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input Table:\n",
    "----------------------------+\n",
    "|Name    | Hobbies           |\n",
    "----------------------------+\n",
    "|John    | Reading, Writing  |\n",
    "|Mary    | Painting, Dancing |\n",
    "|Peter   | Singing, Dancing  |\n",
    "-----------------------------\n",
    "\n",
    "O/p Table:\n",
    "+-----+--------+\n",
    "| Name| Hobbies|\n",
    "+-----+--------+\n",
    "| John| Reading|\n",
    "| John| Writing|\n",
    "| Mary|Painting|\n",
    "| Mary| Dancing|\n",
    "|Peter| Singing|\n",
    "|Peter| Dancing|\n",
    "+-----+--------+\n",
    "\n",
    "\"\"\"\n",
    "from pyspark.sql.functions import split, explode\n",
    "\n",
    "# Sample \n",
    "data = [\n",
    "    (\"John\", \"Reading, Writing\"),\n",
    "    (\"Mary\", \"Painting, Dancing\"),\n",
    "    (\"Peter\", \"Singing, Dancing\")\n",
    "]\n",
    "colums = [\"Name\", \"Hobbies\"]\n",
    "\n",
    "df = spark.createDataFrame(data, colums)\n",
    "\n",
    "df = df.select(\"Name\", explode(split(\"Hobbies\",\",\")).alias(\"Hobbies\"))\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46b3be9c-455e-4554-9f2c-e07961a1da20",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "collect_list"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input:\n",
    "+-----+--------+\n",
    "| Name| Hobbies|\n",
    "+-----+--------+\n",
    "| John| Reading|\n",
    "| John| Writing|\n",
    "| Mary|Painting|\n",
    "| Mary| Dancing|\n",
    "|Peter| Singing|\n",
    "|Peter| Dancing|\n",
    "+-----+--------+\n",
    "O/p:\n",
    "-----------------------------+\n",
    "|Name    | Hobbies           |\n",
    "-----------------------------+\n",
    "|John    | Reading, Writing  |\n",
    "|Mary    | Painting, Dancing |\n",
    "|Peter   | Singing, Dancing  |\n",
    "-----------------------------+\n",
    "SQL:\n",
    "SELECT \n",
    "    Name, \n",
    "    GROUP_CONCAT(Hobbies SEPARATOR ', ') AS Hobbies\n",
    "FROM input_table\n",
    "GROUP BY Name;\n",
    "\"\"\"\n",
    "from pryspark.sql.functions import collect_list, concat_ws\n",
    "df = df.groupBy(\"Name\").agg(concat_ws(\",\", collect_list(df.Hobbies).alias(\"Skills\")))\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4cb21ca-498d-46e0-877f-02d99ce23aed",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "coalesce"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input Table:\n",
    "+-----+-----+-----+\n",
    "|city1|city2|city3|\n",
    "+-----+-----+-----+\n",
    "|Goa  |AP   |     |\n",
    "|     |AP   |NULL |\n",
    "|NULL |     |bglr |\n",
    "+-----+-----+-----+\n",
    "\n",
    "O/p Table: (Ignore Empty & Null values. Put all the first not null value in the last column)\n",
    "+-----+-----+-----+----------------+\n",
    "|city1|city2|city3|FirstNotNullCity|\n",
    "+-----+-----+-----+----------------+\n",
    "|  Goa|   AP|     |             Goa|\n",
    "|     |   AP| NULL|              AP|\n",
    "| NULL|     | bglr|            bglr|\n",
    "+-----+-----+-----+----------------+\n",
    "\n",
    "select city1, city2, city3, coalesce(NULLIF(city1,\"\"), NULLIF(city2,\"\"), NULLIF(city3,\"\")) as FirstNotNullCity from Cities\n",
    "\"\"\"\n",
    "from pyspark.sql.functions import coalesce, when, col\n",
    "data = [(\"Goa\", \"AP\", \"\"), (\"\", \"AP\", None),(None, \"\", \"bglr\")]\n",
    "colums = [\"city1\", \"city2\", \"city3\"]\n",
    "df = spark.createDataFrame(data, colums)\n",
    "df.show(truncate=False)\n",
    "\n",
    "result_df = df.withColumn(\"FirstNotNullCity\", coalesce(when(df.city1 == \"\", None).otherwise(df.city1),\n",
    "                                                       when(df.city2 == \"\", None).otherwise(df.city2),\n",
    "                                                       when(df.city3 == \"\", None).otherwise (df.city3)))\n",
    "\n",
    "result_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c67fa2f-c49f-46d2-b9ae-306255df6698",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Display highest salary from each department\n",
    "+---+----+------+----------+\n",
    "|Id |Name|Salary|Department|\n",
    "+---+----+------+----------+\n",
    "|1  |A   |1000  |IT        |\n",
    "|2  |B   |1500  |IT        |\n",
    "|3  |C   |2500  |IT        |\n",
    "|4  |D   |3000  |HR        |\n",
    "|5  |E   |2000  |HR        |\n",
    "|6  |F   |1000  |HR        |\n",
    "|7  |G   |4000  |Sales     |\n",
    "|8  |H   |4000  |Sales     |\n",
    "|9  |I   |1000  |Sales     |\n",
    "|10 |J   |2000  |Sales     |\n",
    "+---+----+------+----------+\n",
    "\n",
    "Output:\n",
    "+---+----+------+----------+----+\n",
    "| Id|Name|Salary|Department|rank|\n",
    "+---+----+------+----------+----+\n",
    "|  4|   D|  3000|        HR|   1|\n",
    "|  3|   C|  2500|        IT|   1|\n",
    "|  7|   G|  4000|     Sales|   1|\n",
    "|  8|   H|  4000|     Sales|   1|\n",
    "+---+----+------+----------+----+\n",
    "\n",
    "SQL:\n",
    "select id, name, salary, department, rank() over(partition by department order by salary desc) as rank from employee qualify rank=1\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql.functions import dense_rank, col\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "data = [\n",
    "    (1, \"A\", 1000, \"IT\"),\n",
    "    (2, \"B\", 1500, \"IT\"),\n",
    "    (3, \"C\", 2500, \"IT\"),\n",
    "    (4, \"D\", 3000, \"HR\"),\n",
    "    (5, \"E\", 2000, \"HR\"),\n",
    "    (6, \"F\", 1000, \"HR\"),\n",
    "    (7, \"G\", 4000, \"Sales\"),\n",
    "    (8, \"H\", 4000, \"Sales\"),\n",
    "    (9, \"I\", 1000, \"Sales\"),\n",
    "    (10, \"J\", 2000, \"Sales\")\n",
    "]\n",
    "colums = [\"Id\", \"Name\", \"Salary\", \"Department\"]\n",
    "\n",
    "df = spark.createDataFrame(data, colums)\n",
    "\n",
    "rank_df = df.select(\"*\", dense_rank().over(Window.partitionBy(\"Department\").orderBy(col(\"Salary\").desc())).alias(\"rank\"))\n",
    "\n",
    "# display highest rank department wise\n",
    "rank_df= rank_df.filter(rank_df.rank == 1).distinct()\n",
    "rank_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19e40542-cfd6-443a-9b80-a8078fc2b056",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Find age of a person using given DOB"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Find the age of the person with the given dob column in the data frame.\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql.functions import to_date, current_date, floor, months_between, col\n",
    "\n",
    "df = spark.createDataFrame([(\"Gopikrishna\", \"Bangalore\", \"1995-05-27\")], [\"name\", \"city\", \"dob\"])\n",
    "\n",
    "# Step 1: Convert 'dob' string to DateType\n",
    "df = df.withColumn(\"dob\", to_date(col(\"dob\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# Step 2: Calculate age\n",
    "df_with_age = df.withColumn(\"age\", floor(months_between(current_date(), col(\"dob\")) / 12))\n",
    "\n",
    "# Optional: show result\n",
    "df_with_age.select(\"dob\", \"age\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f0d0ce6-dc53-44d1-b157-04d43f19b2f8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "left_anti join"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Find the products that have never been ordered.\n",
    "Dataset: Products (product_id, product_name), Order_Items (order_id, product_id, quantity)\n",
    "\n",
    "SELECT product_id, product_name FROM Products WHERE product_id NOT IN (SELECT product_id FROM Order_Items);\n",
    "\"\"\"\n",
    "# Sample DataFrames\n",
    "products_df = spark.createDataFrame([(1, \"Laptop\"),(2, \"Mouse\"),(3, \"Keyboard\")], [\"product_id\", \"product_name\"])\n",
    "\n",
    "order_items_df = spark.createDataFrame([(101, 1, 2), (102, 2, 5)], [\"order_id\", \"product_id\", \"quantity\"])\n",
    "\n",
    "# LEFT ANTI JOIN → keeps rows from left that have no match in right\n",
    "never_ordered_df = products_df.join(order_items_df, on=\"product_id\", how=\"left_anti\")\n",
    "\n",
    "never_ordered_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da61ccb0-0e8f-47f1-95e6-3e0907bff4ec",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Solving Data Skew Problem"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import spark_partition_id\n",
    "\n",
    "file_path = \"/Volumes/workspace/default/volume/employee_data.csv\"\n",
    "\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Original partition count\n",
    "df.withColumn(\"PartitionId\", spark_partition_id()) \\\n",
    "  .groupBy(\"PartitionId\") \\\n",
    "  .count() \\\n",
    "  .orderBy(\"PartitionId\") \\\n",
    "  .show()\n",
    "\n",
    "# Repartition to 10\n",
    "new_df = df.repartition(10)\n",
    "\n",
    "# New partition count\n",
    "new_df.withColumn(\"PartitionId\", spark_partition_id()) \\\n",
    "      .groupBy(\"PartitionId\") \\\n",
    "      .count() \\\n",
    "      .orderBy(\"PartitionId\") \\\n",
    "      .show()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "from pyspark.sql.functions import spark_partition_id\n",
    "\n",
    "file_path = \"/Volumes/workspace/default/volume/employee_data.csv\"\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "df.rdd.getNumPartitions()\n",
    "df.repartition(10)\n",
    "\n",
    "df.rdd.getNumPartitions()\n",
    "df.withColumn(\"PartitionId\", spark_partition_id()) \\\n",
    "  .groupBy(\"PartitionId\") \\\n",
    "  .count() \\\n",
    "  .orderBy(\"PartitionId\") \\\n",
    "  .show()\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c24b3df-ce01-4ebc-9986-bf964107efe6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Task: Identify products whose total sales revenue has increased every year, including product_id, product_name, and category.\n",
    "\n",
    "Input: sales\n",
    "+----------+----+-------------------+\n",
    "|product_id|year|total_sales_revenue|\n",
    "+----------+----+-------------------+\n",
    "|         1|2021|                100|\n",
    "|         1|2022|                150|\n",
    "|         1|2023|                200|\n",
    "|         2|2021|                300|\n",
    "|         2|2022|                250|\n",
    "|         2|2023|                400|\n",
    "|         3|2021|                 50|\n",
    "|         3|2022|                 50|\n",
    "|         3|2023|                 60|\n",
    "+----------+----+-------------------+\n",
    "Input: products\n",
    "+----------+------------+----------+\n",
    "|product_id|product_name|  category|\n",
    "+----------+------------+----------+\n",
    "|         1|   Product A|Category X|\n",
    "|         2|   Product B|Category Y|\n",
    "|         3|   Product C|Category Z|\n",
    "+----------+------------+----------+\n",
    "\n",
    "Output: Identify products whose total sales revenue has increased every year, including product_id, product_name, and category.\n",
    "+----------+------------+----------+\n",
    "|product_id|product_name|  category|\n",
    "+----------+------------+----------+\n",
    "|         1|   Product A|Category X|\n",
    "+----------+------------+----------+\n",
    "\n",
    "Youtube URL: https://www.youtube.com/watch?v=aHRy8mTzxNs&ab_channel=DEwithDhairy\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql.functions import col, lag\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Sample DataFrames (replace with actual tables)\n",
    "sales_df = spark.createDataFrame([\n",
    " (1, 2019, 1000.00),\n",
    " (1, 2020, 1200.00),\n",
    " (1, 2021, 1100.00),\n",
    " (2, 2019, 500.00),\n",
    " (2, 2020, 600.00),\n",
    " (2, 2021, 900.00),\n",
    " (3, 2019, 300.00),\n",
    " (3, 2020, 450.00),\n",
    " (3, 2021, 400.00)\n",
    "], [\"product_id\", \"year\", \"total_sales_revenue\"])\n",
    "\n",
    "product_df = spark.createDataFrame([\n",
    " (1, 'Laptops', 'Electronics'),\n",
    " (2, 'Jeans', 'Clothing'),\n",
    " (3, 'Chairs', 'Home Appliances')\n",
    " ], [\"product_id\", \"product_name\", \"category\"])\n",
    "\n",
    "# Step 1: Add previous year's revenue\n",
    "sales_with_lag = sales_df.withColumn(\"prev_revenue\", lag(\"total_sales_revenue\").over(Window.partitionBy(\"product_id\").orderBy(\"year\")))\n",
    "\n",
    "# Step 2: Flag whether revenue increased\n",
    "sales_flagged = sales_with_lag.withColumn(\"increased\", (col(\"total_sales_revenue\") - col(\"prev_revenue\")))\n",
    "\n",
    "# Step 3: Filter out products with any non-increasing year\n",
    "valid_products = sales_flagged.groupBy(\"product_id\").agg(min(col(\"increased\"))).filter(min(col(\"increased\") > 0))\n",
    "\n",
    "# Step 4: Join with product details\n",
    "final_result = valid_products.join(products_df, on=\"product_id\", how=\"inner\") \\\n",
    "    .select(\"product_id\", \"product_name\", \"category\")\n",
    "\n",
    "final_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "081cd86a-4578-45c2-8077-e05dad322260",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "subtract vs exceptall"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "subtract - Returns the rows in left DataFrame but not in right DataFrame based on a join of columns. Doesn't include duplicates.\n",
    "exceptall - Returns the rows in left DataFrame but not in right DataFrame based on a join of columns. Includes duplicates as well.\n",
    "\"\"\"\n",
    "# Sample DataFrames\n",
    "df1 = spark.createDataFrame([(1, \"A\"),(1, \"A\"),(2, \"B\"),(3, \"C\")], [\"id\", \"value\"])\n",
    "df2 = spark.createDataFrame([(1, \"A\"),(3, \"C\")], [\"id\", \"value\"])\n",
    "\n",
    "# subtract → removes duplicates from the result\n",
    "df_subtract = df1.subtract(df2)\n",
    "print(\"subtract result (no duplicates):\")\n",
    "df_subtract.show()\n",
    "\n",
    "# exceptAll → keeps duplicates from the left that don't match in right\n",
    "df_exceptall = df1.exceptAll(df2)\n",
    "print(\"exceptAll result (duplicates kept):\")\n",
    "df_exceptall.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "974b8617-2565-4dcc-89d7-309db0856672",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Handle bad data: PERMISSIVE, DROPMALFORMED, FAILFAST"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "PERMISSIVE (default) =>\tCorrupt/malformed rows go into a special column _corrupt_record (if columnNameOfCorruptRecord is set), and other columns are set to null.\n",
    "DROPMALFORMED\t     => Entire bad rows are silently dropped.\n",
    "FAILFAST\t         => Job immediately fails if a bad row is found.\n",
    "\"\"\"\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Example file path\n",
    "file_path = \"/Volumes/workspace/default/volume/employee_data.csv\"\n",
    "\n",
    "# PERMISSIVE mode (default), columnNameOfCorruptRecord APPEND new column and puts faulty rows in it.\n",
    "df_permissive = spark.read.csv(file_path, header=True, schema=schema, mode=\"PERMISSIVE\", \n",
    "                               columnNameOfCorruptRecord=\"_corrupt_record\")\n",
    "print(\"PERMISSIVE mode result:\")\n",
    "df_permissive.show(truncate=False)\n",
    "\n",
    "# DROPMALFORMED mode, drops the faulty rows\n",
    "df_drop = spark.read.csv(file_path, header=True, schema=schema, mode=\"DROPMALFORMED\")\n",
    "print(\"DROPMALFORMED mode result:\")\n",
    "df_drop.show()\n",
    "\n",
    "# FAILFAST mode\n",
    "# This will throw an exception if a bad row is found\n",
    "df_failfast = spark.read.csv(file_path, header=True, schema=schema, mode=\"FAILFAST\")\n",
    "df_failfast.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1995e838-2f7c-4055-9ac9-6315feb307f6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "lag & lead"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "lag(column, offset) → gets the value from a previous row within a window.\n",
    "lead(column, offset) → gets the value from a next row within a window.\n",
    "\"\"\"\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag, lead, col, when\n",
    "\n",
    "# Example telecom performance data\n",
    "data = [\n",
    "    (\"SITE_A\", \"2024-09-01\", 4500),\n",
    "    (\"SITE_A\", \"2024-09-02\", 4800),\n",
    "    (\"SITE_A\", \"2024-09-03\", 4300),\n",
    "    (\"SITE_B\", \"2024-09-01\", 5200),\n",
    "    (\"SITE_B\", \"2024-09-02\", 5400),\n",
    "    (\"SITE_B\", \"2024-09-03\", 5600),\n",
    "]\n",
    "columns = [\"site_id\", \"date\", \"traffic_gb\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Window partitioned by site and ordered by date\n",
    "w = Window.partitionBy(\"site_id\").orderBy(\"date\")\n",
    "\n",
    "# Add previous and next day's traffic\n",
    "df_trend = (\n",
    "    df.withColumn(\"prev_day_traffic\", lag(\"traffic_gb\", 1).over(w))\n",
    "      .withColumn(\"next_day_traffic\", lead(\"traffic_gb\", 1).over(w))\n",
    "      .withColumn(\n",
    "          \"traffic_change_gb\",\n",
    "          col(\"traffic_gb\") - col(\"prev_day_traffic\")\n",
    "      )\n",
    "      .withColumn(\n",
    "          \"spike_flag\",\n",
    "          when(col(\"traffic_gb\") - col(\"prev_day_traffic\") > 300, \"SPIKE\").otherwise(\"NORMAL\")\n",
    "      )\n",
    ")\n",
    "\n",
    "df_trend.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0e5dce8-c1ac-4155-9ee7-977a4e36400c",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"col\":255},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1755099333807}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "Infosys"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "input:\n",
    "--------\n",
    "arrayArrayData = [\n",
    "  (\"James\",[[\"Java\",\"Scala\",\"C++\"],[\"Spark\",\"Java\"]]),\n",
    "  (\"Michael\",[[\"Spark\",\"Java\",\"C++\"],[\"Spark\",\"Java\"]]),\n",
    "  (\"Robert\",[[\"CSharp\",\"VB\"],[\"Spark\",\"Python\"]])\n",
    "]\n",
    " \n",
    "output:\n",
    "-------\n",
    "+-------+------------------+\n",
    "|name   |colName           |\n",
    "+-------+------------------+\n",
    "|James  |[Java, Scala, C++]|\n",
    "|James  |[Spark, Java]     |\n",
    "|Michael|[Spark, Java, C++]|\n",
    "|Michael|[Spark, Java]     |\n",
    "|Robert |[CSharp, VB]      |\n",
    "|Robert |[Spark, Python]   |\n",
    "+-------+------------------+\n",
    "\"\"\"\n",
    "from pyspark.sql.functions import explode, col\n",
    "# Input data\n",
    "arrayArrayData = [\n",
    "    (\"James\", [[\"Java\", \"Scala\", \"C++\"], [\"Spark\", \"Java\"]]),\n",
    "    (\"Michael\", [[\"Spark\", \"Java\", \"C++\"], [\"Spark\", \"Java\"]]),\n",
    "    (\"Robert\", [[\"CSharp\", \"VB\"], [\"Spark\", \"Python\"]])\n",
    "]\n",
    "df = spark.createDataFrame(arrayArrayData, [\"name\", \"courses\"])\n",
    "\n",
    "# Flatten the array of arrays\n",
    "df_flattened = df.select(\"name\", explode(col(\"courses\")))\n",
    "display(df_flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "662dea32-a992-4d63-9e42-760211c10fc2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Infosys"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input\n",
    "-----\n",
    "col1      col2       col3\n",
    "a         aa         1\n",
    "a         aa         2\n",
    "b         bb         3\n",
    "b         bb         4\n",
    "b         bb         5\n",
    " \n",
    "output\n",
    "-------\n",
    "col1     col2        col3\n",
    "a        aa          [1,2]\n",
    "b        bb          [3,4,5]\n",
    "\"\"\"\n",
    "from pyspark.sql.functions import collect_list\n",
    "\n",
    "# Input data\n",
    "data = [(\"a\", \"aa\", 1), (\"a\", \"aa\", 2), (\"b\", \"bb\", 3), (\"b\", \"bb\", 4), (\"b\", \"bb\", 5)]\n",
    "schema = [\"col1\", \"col2\", \"col3\"]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df=df.groupBy(\"col1\", \"col2\").agg(collect_list(\"col3\").alias(\"col3\"))\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84acab4b-c418-4547-b1af-7072dcc5276e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "UST Global"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "show emp_name, emp_deptid, state where there is same state & same department\n",
    "\n",
    "-- Employee Table --\n",
    "+------+--------+-----------+\n",
    "|emp_id|emp_name|empdept_id |\n",
    "+------+--------+-----------+\n",
    "|     1|   Harry|          5|\n",
    "|     2|    Ron |          5|\n",
    "|     3| Neville|         10|\n",
    "|     4|  Malfoy|          5|\n",
    "+------+--------+-----------+\n",
    "-- Address Table --\n",
    "+------+--------------+------+\n",
    "|emp_id| city         |state |\n",
    "+------+--------------+------+\n",
    "|1     | Nagpur       |   MH |\n",
    "|2     | Pune         |   MH |\n",
    "|3     | Mumbai       |   MH |\n",
    "|4     | Chennai      |   TN |\n",
    "+------+--------------+------+\n",
    "\"\"\"\n",
    "\n",
    "# Sample data\n",
    "employee_data = [(1, \"Harry\", 5),(2, \"Ron\", 5),(3, \"Neville\", 10),(4, \"Malfoy\", 5)]\n",
    "address_data = [(1, \"Nagpur\", \"MH\"),(2, \"Pune\", \"MH\"), (3, \"Mumbai\", \"MH\"),(4, \"Chennai\", \"TN\")]\n",
    "\n",
    "# Create DataFrames\n",
    "df_emp = spark.createDataFrame(employee_data, [\"emp_id\", \"emp_name\", \"empdept_id\"])\n",
    "df_addr = spark.createDataFrame(address_data, [\"emp_id\", \"city\", \"state\"])\n",
    "\n",
    "# Join\n",
    "df_joined = df_emp.join(df_addr, \"emp_id\")\n",
    "\n",
    "# Find dept_id + state combinations having more than 1 employee\n",
    "df_filtered_keys = (\n",
    "    df_joined.groupBy(\"empdept_id\", \"state\")\n",
    "             .agg(count(\"*\").alias(\"cnt\"))\n",
    "             .filter(col(\"cnt\") > 1)\n",
    "             .select(\"empdept_id\", \"state\")\n",
    ")\n",
    "\n",
    "# Join back to get full details\n",
    "result_df = (\n",
    "    df_joined.join(df_filtered_keys, [\"empdept_id\", \"state\"])\n",
    "             .select(\"emp_name\", \"empdept_id\", \"state\")\n",
    ")\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "856e4c77-2222-4d25-afbf-c9a89a5dd660",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "UST Global"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Customers Table:\n",
    "---------------\n",
    "Customer_ID Name        City\n",
    "1           John Levi   New York\n",
    "2           Jane Tye    Los Angeles\n",
    "3           Mike Foley  Chicago\n",
    "4           Alice White New York\n",
    "\n",
    "Orders Table:\n",
    "---------------\n",
    "Order_ID    Customer_ID Order_Date  Order Total\n",
    "100         1           2023-07-01  100.00\n",
    "101         2           2023-06-15  50.00\n",
    "102         3           2023-07-05  150.00\n",
    "103         1           2023-07-07  75.00\n",
    "104         4           2023-07-02  200.00\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "SELECT \n",
    "    c.Name,\n",
    "    COUNT(o.Order_ID) AS total_orders\n",
    "FROM Customers c\n",
    "JOIN Orders o \n",
    "    ON c.Customer_ID = o.Customer_ID\n",
    "WHERE MONTH(o.Order_Date) <> 6  -- Exclude June\n",
    "GROUP BY c.Name;\n",
    "\n",
    "\"\"\"\n",
    "customers_data = [\n",
    "    (1, \"John Levi\", \"New York\"),\n",
    "    (2, \"Jane Tye\", \"Los Angeles\"),\n",
    "    (3, \"Mike Foley\", \"Chicago\"),\n",
    "    (4, \"Alice White\", \"New York\")\n",
    "]\n",
    "orders_data = [\n",
    "    (100, 1, \"2023-07-01\", 100.00),\n",
    "    (101, 2, \"2023-06-15\", 50.00),\n",
    "    (102, 3, \"2023-07-05\", 150.00),\n",
    "    (103, 1, \"2023-07-07\", 75.00),\n",
    "    (104, 4, \"2023-07-02\", 200.00)\n",
    "]\n",
    "\n",
    "from pyspark.sql.functions import month, count\n",
    "# Create DataFrames\n",
    "df_customers = spark.createDataFrame(customers_data, [\"Customer_ID\", \"Name\", \"City\"])\n",
    "df_orders = spark.createDataFrame(orders_data, [\"Order_ID\", \"Customer_ID\", \"Order_Date\", \"Order_Total\"])\n",
    "\n",
    "# Filter out June orders, group by customer, count orders\n",
    "result_df = (\n",
    "    df_customers.join(df_orders, \"Customer_ID\")\n",
    "    .filter(month(col(\"Order_Date\")) != 6)  # Exclude June\n",
    "    .groupBy(\"Name\")\n",
    "    .agg(count(\"Order_ID\").alias(\"total_orders\"))\n",
    ")\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9aabec15-41b0-41ea-8e17-e63e6b19f2f6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Capgemini: Writing to files"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "id, age -> Store in a file\n",
    "name, gender -> Store in another file\n",
    "marks -> Store in another file\n",
    "\"\"\"\n",
    "data = [\n",
    "    (1, \"Sagar\", 23, \"Male\", 68.0),\n",
    "    (2, \"Kim\", 35, \"Female\", 90.2),\n",
    "    (3, \"Alex\", 40, \"Male\", 79.1)\n",
    "]\n",
    "\n",
    "#schema = \"Id int, Name string, Age int, Gender string, Marks float\"\n",
    "schema = [\"id\", \"name\", \"age\", \"gender\", \"marks\"]\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "int_df = df.select(\"Id\", \"Age\")\n",
    "int_df.write.mode(\"overwrite\").parquet(\"path/integers\")\n",
    "\n",
    "str_df = df.select(\"Name\", \"Gender\")\n",
    "str_df.write.mode(\"overwrite\").parquet(\"path/strings\")\n",
    "\n",
    "float_df = df.select(\"Marks\")\n",
    "float_df.write.mode(\"overwrite\").parquet(\"path/floats\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "set_of_dtypes = set([i[1] for i in df.dtypes])\n",
    "\n",
    "for i in set_of_dtypes:\n",
    "    cols = []\n",
    "    for j in df.dtypes:\n",
    "        if i == j[1]:\n",
    "            cols.append(j[0])\n",
    "    df.select(cols).write.mode('overwrite').save(f'/path/{i}')\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf2daa80-0803-4b4f-b615-f60cfe518b71",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Capgemini: Lift problem"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Inputs:\n",
    "================================================\n",
    "Lift Capacity\n",
    "-----------------\n",
    "| id | capacity |\n",
    "| -- | -------- |\n",
    "| 1  | 300      |\n",
    "| 2  | 350      |\n",
    "\n",
    "Passenger\n",
    "------------------------------------------\n",
    "| passenger_name  | weigh_kg   | lift_id  |\n",
    "| --------------- | ---------- | -------- |\n",
    "| Rahul           | 85         | 1        |\n",
    "| Adarsh          | 73         | 1        |\n",
    "| Riti            | 95         | 1        |\n",
    "| Viraj           | 80         | 1        |\n",
    "| Vimal           | 83         | 2        |\n",
    "| Neha            | 77         | 2        |\n",
    "| Priti           | 73         | 2        |\n",
    "| Himanshi        | 85         | 2        |\n",
    "\n",
    "Output:\n",
    "==================================================\n",
    "| lift_id  | names                     |\n",
    "| -------- | ------------------------- |\n",
    "| 1        | Adarsh,Viraj,Rahul        |\n",
    "| 2        | Priti,Neha,Vimal,Himanshi |\n",
    "\n",
    "Task:\n",
    "Produce a comma-separated list of passengers who can be accommodated in each lift without exceeding the weight capacity of that lift.\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import sum, col, collect_list\n",
    "\n",
    "# Data\n",
    "lift_data = [(1, 300), (2, 350)]\n",
    "passenger_data = [\n",
    "                    (\"Rahul\", 85, 1),\n",
    "                    (\"Adarsh\", 73, 1),\n",
    "                    (\"Riti\", 95, 1),\n",
    "                    (\"Viraj\", 80, 1),\n",
    "                    (\"Vimal\", 83, 2),\n",
    "                    (\"Neha\", 77, 2),\n",
    "                    (\"Priti\", 73, 2),\n",
    "                    (\"Himanshi\", 85, 2)\n",
    "                ]\n",
    "\n",
    "# Schema\n",
    "lift_schema = \"id int, capacity_kg int\"\n",
    "passenger_schema = \"passenger_name string, weight_kg int, lift_id int\"\n",
    "\n",
    "# DFs\n",
    "lift_df = spark.createDataFrame(lift_data, lift_schema)\n",
    "passenger_df = spark.createDataFrame(passenger_data, passenger_schema)\n",
    "\n",
    "# Transformations\n",
    "join_df = lift_df.join(passenger_df, lift_df.id == passenger_df.lift_id, \"inner\").drop(\"id\")\n",
    "weights_df = join_df.withColumn(\"weights\", sum(col(\"weight_kg\")).over(Window.partitionBy(\"lift_id\").orderBy(\"weight_kg\")))\n",
    "capacity_df = weights_df.filter(weights_df.weights <= weights_df.capacity_kg)\n",
    "\n",
    "final_df = capacity_df.groupBy(\"lift_id\").agg(collect_list(\"passenger_name\").alias(\"passenger_names\"))\n",
    "\n",
    "display(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b80c9b11-30b2-4c6d-a649-05234bb33cbd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "LTI Mindtree"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input: Fill NULL values wih the first non-null value in the column.\n",
    "+------------+-------------+\n",
    "| category   | brand name  |\n",
    "| ---------- | ----------- |\n",
    "| chocolates | 5-star      |\n",
    "| NULL       | dairy milk  |\n",
    "| NULL       | perk        |\n",
    "| NULL       | eclair      |\n",
    "| Biscuits   | Britania    |\n",
    "| NULL       | good day    |\n",
    "| NULL       | boost       |\n",
    "+------------+-------------+\n",
    "Output:\n",
    "+------------+-------------+\n",
    "| category   | brand name  |\n",
    "| ---------- | ----------- |\n",
    "| chocolates | 5-star      |\n",
    "| chocolates | dairy milk  |\n",
    "| chocolates | perk        |\n",
    "| chocolates | eclair      |\n",
    "| Biscuits   | Britania    |\n",
    "| Biscuits   | good day    |\n",
    "| Biscuits   | boost       |\n",
    "+------------+-------------+\n",
    "\n",
    "SELECT \n",
    "    LAST_VALUE(category IGNORE NULLS) \n",
    "        OVER (ORDER BY brand_name ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS category,\n",
    "    brand_name\n",
    "FROM products;\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql.functions import col, last\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Data\n",
    "data = [\n",
    "    (\"chocolates\", \"5-star\"),\n",
    "    (None, \"dairy milk\"),\n",
    "    (None, \"perk\"),\n",
    "    (None, \"eclair\"),\n",
    "    (\"Biscuits\", \"Britania\"),\n",
    "    (None, \"good day\"),\n",
    "    (None, \"boost\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"category\", \"brand_name\"])\n",
    "\n",
    "# Define window spec\n",
    "w = Window.orderBy(\"brand_name\").rowsBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "# Forward fill using last() with ignoreNulls=True\n",
    "df_filled = df.withColumn(\"category_filled\", last(\"category\", True).over(w)) \\\n",
    "              .select(col(\"category_filled\").alias(\"category\"), \"brand_name\")\n",
    "\n",
    "df_filled.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cdf006cc-d7dd-4ea2-a7d3-fe3e9734c08c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Capgemini"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input:\n",
    "+------------------+------------------+--------------------+----------------+\n",
    "| transaction_type | transaction_date | transaction_amount | transaction_id |\n",
    "+------------------+------------------+--------------------+----------------+\n",
    "| credit           | 01-07-25         | 1000               | 1              |\n",
    "| debit            | 02-07-25         | 200                | 2              |\n",
    "| credit           | 03-07-25         | 500                | 3              |\n",
    "+------------------+------------------+--------------------+----------------+\n",
    "Output:\n",
    "+------------------+----------------+--------+-------+---------+\n",
    "| transaction_date | transaction_id | credit | debit | balance |\n",
    "+------------------+----------------+--------+-------+---------+\n",
    "| 01-07-25         | 1              | 1000   | NULL  | 1000    |\n",
    "| 02-07-25         | 2              | NULL   | 200   | 800     |\n",
    "| 03-07-25         | 3              | 500    | NULL  | 1300    |\n",
    "+------------------+----------------+--------+-------+---------+\n",
    "\n",
    "SELECT\n",
    "  transaction_date,\n",
    "  transaction_id,\n",
    "  CASE WHEN transaction_type = 'credit' THEN transaction_amount END AS credit,\n",
    "  CASE WHEN transaction_type = 'debit' THEN transaction_amount END AS debit,\n",
    "  SUM(CASE \n",
    "        WHEN transaction_type = 'credit' THEN transaction_amount \n",
    "        WHEN transaction_type = 'debit' THEN -transaction_amount \n",
    "      END) OVER (\n",
    "        ORDER BY transaction_date, transaction_id\n",
    "        ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n",
    "      ) AS balance\n",
    "FROM transactions;\n",
    "\"\"\"\n",
    "from pyspark.sql.functions import col, when, lit, sum\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (\"credit\", \"01-07-25\", 1000, 1),\n",
    "    (\"debit\", \"02-07-25\", 200, 2),\n",
    "    (\"credit\", \"03-07-25\", 500, 3)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"transaction_type\", \"transaction_date\", \"transaction_amount\", \"transaction_id\"])\n",
    "\n",
    "# Add credit and debit columns\n",
    "df = df.withColumn(\"credit\", when(col(\"transaction_type\") == \"credit\", col(\"transaction_amount\")).otherwise(lit(0)))\\\n",
    "        .withColumn(\"debit\", when(col(\"transaction_type\") == \"debit\", col(\"transaction_amount\")).otherwise(lit(0)))\n",
    "\n",
    "# Define window for running balance\n",
    "window_spec = Window.orderBy(\"transaction_date\", \"transaction_id\").rowsBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "# Compute running balance\n",
    "df = df.withColumn(\"balance\", sum(col(\"credit\") - col(\"debit\")).over(window_spec))\n",
    "\n",
    "# Select final columns\n",
    "df_final = df.select(\"transaction_date\", \"transaction_id\", \"credit\", \"debit\", \"balance\")\n",
    "df_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2786ecb9-52fe-4712-8d40-1bee12e175d3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Wells Fargo"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Flatten JSON file given: complex json file: flatten this file & save it as a parquet file s3,\n",
    "\n",
    "\"customer\": {\n",
    "    \"customerId\": \"CUST-1001\",\n",
    "    \"name\": {\n",
    "      \"first\": \"Aarav\",\n",
    "      \"last\": \"Sharma\"\n",
    "    },\n",
    "    \"email\": \"aarav.sharma@example.com\",\n",
    "    \"phone\": \"+91-9876543210\",\n",
    "    \"address\": {\n",
    "        \"line1\": \"123 MG Road\",\n",
    "        \"line2\": \"Indiranagar\",\n",
    "        \"city\": \"Bengaluru\",\n",
    "        \"state\": \"KA\",\n",
    "        \"postalCode\": \"560038\",\n",
    "        \"country\": \"India\"\n",
    "    }\n",
    "  }\n",
    "\"\"\"\n",
    "spark = SparkSession.builder.appName(\"App\").getOrCreate()\n",
    "df = spark.read.json(\"s3a://bucket/input.json\", multiline=true)\n",
    "df.printSchema()\n",
    "flat_df = df.select(col(\"customer.customerid\").alias(\"customerid)),\n",
    "\t\t\tcol(\"customer.name.first\").alias(\"firstName\")\n",
    ")\n",
    "flat_df.write.mode(\"overwrite\").parquet(\"s3a://bucket/location/customer_pq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d60e767d-0e45-494f-84fa-cd3565747b32",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Wells Fargo"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "You have student, score cols in a table. Obtain max score first.\n",
    "\n",
    "Input:\n",
    "student, score\n",
    "Alece 95\n",
    "John 96\n",
    "Tom 84\n",
    "Rechard 87\n",
    "\n",
    "output:\n",
    "student, score\n",
    "John 96\n",
    "Alece 95\n",
    "Rechard 87\n",
    "Tom 84\n",
    "\"\"\"\n",
    "result = df.orderBy(col(\"score\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be76f2a5-43c1-4c97-8a5e-dd9581813f2b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Synechron"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Read the JSON file  {first_name:\"XYZ\", last_name:\"YZX\"}\n",
    "First_name check for NOT_NULL, if it NULL then DROP those records\n",
    "Derive new column FULL_NAME by concatenating both columns\n",
    "Display all 3 columns only when the FULL_NAME is greater than 30 characters\n",
    "\"\"\"\n",
    "df_json = spark.read.json(\"../Test_Files/file.json\")\n",
    "df_filter = df_json.filter(col(\"first_name\").isNotNull())\n",
    "df_full_name = df_filter.withColumn(\"full_name\", concat_ws(\" \", col(\"first_name\"), col(\"last_name\")))\n",
    "df_result = df_full_name.filter(length(col(\"full_name\"))>30).\\\n",
    "    select(\"first_name\", \"last_name\", \"full_name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6dd64a8f-4a42-4f22-82a6-0a088e9f584d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "LTI MIndtree"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input:\n",
    "trips table:\n",
    "trip_id\n",
    "client_id\t=> foreign key user_id from users\n",
    "driver_id\t=> foreign key user_id from users\n",
    "status\t\t=> values 'cancelled', 'active', 'completed'\n",
    "request_dt\t\n",
    "\n",
    "users table:\n",
    "user_id\n",
    "banned_status\t=> 'yes','no'\n",
    "role \t\t=> 'driver','client'\n",
    "\n",
    "cancellation_rate=total_num_of_rides/cancelled_num_of_rides\n",
    "PS: only consider users with banned_status='No'\n",
    " \n",
    "Output:\n",
    "request_dt & cancellation_rate\n",
    " \n",
    "\"\"\"\n",
    "from pyspark.sql.functions import col, when, sum as _sum, count, round\n",
    "\n",
    "# Non-banned clients\n",
    "clients = users.filter((col(\"role\") == \"client\") & (col(\"banned_status\") == \"no\")) \\\n",
    "               .select(col(\"user_id\").alias(\"client_id\"))\n",
    "\n",
    "# Non-banned drivers\n",
    "drivers = users.filter((col(\"role\") == \"driver\") & (col(\"banned_status\") == \"no\")) \\\n",
    "               .select(col(\"user_id\").alias(\"driver_id\"))\n",
    "\n",
    "# Valid trips (only non-banned client & driver)\n",
    "valid_trips = (trips\n",
    "               .join(clients, \"client_id\")\n",
    "               .join(drivers, \"driver_id\"))\n",
    "\n",
    "# Step 1: Add a flag column for cancelled rides\n",
    "valid_trips = valid_trips.withColumn(\n",
    "    \"is_cancelled\",\n",
    "    when(col(\"status\") == \"cancelled\", 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# Step 2: Aggregate total rides and cancelled rides\n",
    "agg_df = (valid_trips\n",
    "          .groupBy(\"request_dt\")\n",
    "          .agg(\n",
    "              count(\"*\").alias(\"total_rides\"),\n",
    "              _sum(\"is_cancelled\").alias(\"cancelled_rides\")\n",
    "          ))\n",
    "\n",
    "# Step 3: Compute cancellation_rate = total / cancelled\n",
    "result = agg_df.withColumn(\n",
    "    \"cancellation_rate\",\n",
    "    round((col(\"total_rides\") * 1.0) / col(\"cancelled_rides\"), 2)\n",
    ").orderBy(\"request_dt\")\n",
    "\n",
    "result.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PySpark_Notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
