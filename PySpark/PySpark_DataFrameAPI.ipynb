{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14205638-8b6b-4a37-b946-ca227e3aa35d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### PySpark Actions on DataFrames\n",
    "In PySpark, actions on a DataFrame are operations that trigger execution of the underlying lazy transformations and return a result to the driver or write to storage.\n",
    "\n",
    "| Action                  | Description                                                                      | Returns to Driver?  |\n",
    "| ----------------------- | -------------------------------------------------------------------------------- | ------------------- |\n",
    "| `show()`                | Displays top rows in a tabular format                                            | ✅ (for display)     |\n",
    "| `collect()`             | Returns **all rows** as a list to the driver (use cautiously on large datasets!) | ✅                   |\n",
    "| `take(n)`               | Returns the first `n` rows as a list                                             | ✅                   |\n",
    "| `head(n)` / `first()`   | Alias for `take(n)` / `take(1)[0]`                                               | ✅                   |\n",
    "| `count()`               | Returns the number of rows in the DataFrame                                      | ✅                   |\n",
    "| `describe()`            | Returns statistics summary for numeric columns (mean, stddev, min, max, etc.)    | ✅ (as DataFrame)    |\n",
    "| `show(n)`               | Displays first `n` rows                                                          | ✅                   |\n",
    "| `toPandas()`            | Converts entire DataFrame to a Pandas DataFrame on the driver                    | ✅ (use cautiously!) |\n",
    "| `write()`               | Saves the DataFrame to file systems (CSV, Parquet, etc.)                         | ❌                   |\n",
    "| `foreach()`             | Executes a function on each row (like a for-loop) – side effects only            | ❌                   |\n",
    "| `foreachPartition()`    | Similar to `foreach()` but operates per partition – better for efficiency        | ❌                   |\n",
    "| `cache()` / `persist()` | Not actions themselves, but force materialization when followed by an action     | ❌ (until used)      |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf8ed89f-87af-40f6-be54-a1157d632b10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add rows\n",
    "row1 = (\"Ram\", 30, \"Senior Engineer\", \"India\", 100000)\n",
    "row2 = (\"Krishna\", 25, \"Junior Engineer\", \"India\", 50000)\n",
    "row3 = (\"Sree\", 35, \"Senior Engineer\", \"India\", 40000)  \n",
    "data = [row1, row2, row3]\n",
    "\n",
    "# Define Colums\n",
    "colums = [\"name\", \"age\", \"role\", \"location\", \"salary\"]\n",
    "df = spark.createDataFrame(data, colums) # Create data \n",
    "\n",
    "# Add new row\n",
    "new_row = [(\"Siva\", 28, \"Senior Engineer\", \"India\", 60000)]\n",
    "new_row_df = spark.createDataFrame(new_row, colums)\n",
    "df = df.union(new_row_df)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f03306e-6f24-4937-9349-cb075577f4f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Put location to Hyderabad for Sree and rest to Bangalore\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"location\", lit(\"India\")\n",
    ")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3eeea3f4-80a2-4801-9000-d9b9ffdb4967",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1753943547471}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb7b278c-bbec-4c61-bfa7-4bc2836a1f93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5424d9ea-3bea-49b3-886a-59e8a2e2aa4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.select(\"age\", \"role\").display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d661695-76ee-4ae1-a0ed-b2b68182b9ef",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1753943840219}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.select(when(df[\"age\"]>25, df[\"name\"])).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a02ed553-55be-4514-8974-41a4b04eb06e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.select(when(df[\"age\"] > 25, df[\"name\"])).na.drop().display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "616addd4-3425-4dfe-9649-fd9543bd8eb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.orderBy(df[\"salary\"].desc()).display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PySpark_DataFrameAPI",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
