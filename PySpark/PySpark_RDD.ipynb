{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1b9b50e-e91b-4119-b7ed-79ad8a7ce86d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ## # **RDD Transformations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3874b3c3-1ff5-4b19-9dfe-069eb0a4bc6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**NARROW TRANSFORMATIONS (No Shuffle):**\n",
    "\n",
    "These transformations operate within a single partition, so no data is moved across the network.\n",
    "\n",
    "**WIDE TRANSFORMATIONS (Causes Shuffle):**\n",
    "\n",
    "These transformations re-partition the data — often across the cluster — and involve network I/O.\n",
    "\n",
    "| **Transformation** | **Type** | **Causes Shuffle?** |\n",
    "| ------------------ | -------- | ------------------- |\n",
    "| `map()`            | Narrow   | ❌ No                |\n",
    "| `filter()`         | Narrow   | ❌ No                |\n",
    "| `flatMap()`        | Narrow   | ❌ No                |\n",
    "| `reduceByKey()`    | Wide     | ✅ Yes               |\n",
    "| `groupByKey()`     | Wide     | ✅ Yes               |\n",
    "| `join()`           | Wide     | ✅ Yes               |\n",
    "| `distinct()`       | Wide     | ✅ Yes               |\n",
    "\n",
    "**How About DataFrame Transformations?**\n",
    "\n",
    "DataFrames in PySpark have their own methods (e.g., select(), withColumn(), groupBy(), join(), dropDuplicates()), but under the hood, many of them compile down to RDD operations and get optimized using the Catalyst optimizer.\n",
    "\n",
    "| **RDD Operation** | **DataFrame Equivalent**          |\n",
    "| ----------------- | --------------------------------- |\n",
    "| `map()`           | `select()`, `withColumn()`        |\n",
    "| `filter()`        | `filter()` / `where()`            |\n",
    "| `flatMap()`       | `explode()`                       |\n",
    "| `reduceByKey()`   | `groupBy().agg()` with reduction  |\n",
    "| `groupByKey()`    | `groupBy()`                       |\n",
    "| `join()`          | `join()`                          |\n",
    "| `distinct()`      | `dropDuplicates()` / `distinct()` |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e50e865b-eb5c-4fd5-9835-98e5e344d53c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "RDD Creation"
    }
   },
   "outputs": [],
   "source": [
    "# Create Spark Context for rdd\n",
    "rdd = spark.sparkContext.parallelize([1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fddfd391-cdf5-4a9f-9af1-fbd688481da2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "RDD Transformations"
    }
   },
   "outputs": [],
   "source": [
    "# map()\n",
    "rdd.map(lambda x: x * 2).collect()  # [2, 4, 6, 8]\n",
    "\n",
    "# filter()\n",
    "rdd.filter(lambda x: x % 2 == 0).collect()  # [2, 4]\n",
    "\n",
    "# flatMap()\n",
    "rdd = spark.sparkContext.parallelize([\"hello world\", \"spark rdd\"])\n",
    "rdd.flatMap(lambda x: x.split()).collect()  # ['hello', 'world', 'spark', 'rdd']\n",
    "\n",
    "\"\"\"\n",
    "Actions:\n",
    "collect(), count(), first(), take(n)\n",
    "\"\"\"\n",
    "rdd.collect()  # [1, 2, 3, 4]\n",
    "rdd.count()  # 4\n",
    "rdd.first()  # 1\n",
    "rdd.take(2)  # [1, 2]"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PySpark_RDD",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
