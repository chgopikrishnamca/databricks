{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8637e054-d2c9-4bcc-84b4-c1028eca2198",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Streaming S3"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StringType, IntegerType, TimestampType\n",
    "from pyspark.sql.functions import col, lit, window, when, round, concat_ws\n",
    "\n",
    "# Step 1: Create Spark session\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Streaming Application\")\n",
    "    .enableHiveSupport()\n",
    "    .config(\"spark.dynamicAllocation.minExecutors\", 12)  # 2 is generally recommended\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", 30)  # 2 is generally recommended\n",
    "    .config(\"spark.executor.memory\", \"8g\")  # default is 3 - adjust if needed\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Step 2: Define schema\n",
    "schema = StructType() \\\n",
    "    .add(\"id\", StringType()) \\\n",
    "    .add(\"first_name\", StringType()) \\\n",
    "    .add(\"last_name\", StringType()) \\\n",
    "    .add(\"role\", StringType()) \\\n",
    "    .add(\"salary\", IntegerType()) \\\n",
    "    .add(\"event_time\", TimestampType())\n",
    "\n",
    "# Step 3: Read streaming XML data from S3\n",
    "xml_df = spark.readStream \\\n",
    "    .format(\"xml\") \\\n",
    "    .option(\"rowTag\", \"employee\") \\\n",
    "    .schema(schema) \\\n",
    "    .load(\"s3a://your-bucket/input-xml/\")\n",
    "\n",
    "# Step 4: Transformations\n",
    "transformed_df = xml_df \\\n",
    "    .withColumn(\"name\", concat_ws(\" \", col(\"first_name\"), col(\"last_name\"))) \\\n",
    "    .drop(\"first_name\", \"last_name\") \\\n",
    "    .withColumn(\"bonus\", round(\n",
    "        when(col(\"role\") == \"ENGINEER\", col(\"salary\") * 0.15)\n",
    "        .when(col(\"role\") == \"ANALYST\", col(\"salary\") * 0.10)\n",
    "        .otherwise(0), 2)) \\\n",
    "    .withColumn(\"source\", lit(\"S3_XML_STREAM\"))\n",
    "\n",
    "# Step 5: Windowed aggregation\n",
    "windowed_df = transformed_df \\\n",
    "    .withWatermark(\"event_time\", \"10 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"event_time\"), \"10 minutes\"),\n",
    "        col(\"role\")\n",
    "    ) \\\n",
    "    .agg(\n",
    "        {\"*\": \"count\", \"bonus\": \"sum\"}\n",
    "    ) \\\n",
    "k   # Step 6: Write results to S3 in Parquet format\n",
    "query = windowed_df.writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", \"s3a://your-bucket/output-bonus-windowed/\") \\\n",
    "    .option(\"checkpointLocation\", \"s3a://your-bucket/checkpoints/bonus-job/\") \\\n",
    "    .outputMode(\"append\") \\ \n",
    "    .start()\n",
    "\n",
    "# Step 7: Wait for termination\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d030066c-4228-447f-a57f-b6b1f36bc015",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Streaming Kafka"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, window, from_json, to_json\n",
    "from pyspark.sql.types import StructType, StringType, DoubleType, TimestampType\n",
    "\n",
    "# 1. Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaWindowWatermarkExample\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 2. Schema for JSON messages\n",
    "schema = StructType() \\\n",
    "    .add(\"device_id\", StringType()) \\\n",
    "    .add(\"temperature\", DoubleType()) \\\n",
    "    .add(\"event_time\", TimestampType())  # event timestamp\n",
    "\n",
    "# 3. Read stream from Kafka\n",
    "df_raw = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"input_topic\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "# 4. Parse JSON and extract columns\n",
    "df_parsed = df_raw.select(\n",
    "    from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\")\n",
    ").select(\"data.*\")\n",
    "\n",
    "# 5. Apply watermark and window\n",
    "# Watermark: Wait up to 10 minutes for late data\n",
    "# Window: Group events into 5-minute intervals\n",
    "df_windowed = df_parsed \\\n",
    "    .withWatermark(\"event_time\", \"10 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"event_time\"), \"5 minutes\"),\n",
    "        col(\"device_id\")\n",
    "    ) \\\n",
    "    .avg(\"temperature\") \\\n",
    "    .withColumnRenamed(\"avg(temperature)\", \"avg_temp\")\n",
    "\n",
    "# 6. Prepare output for Kafka\n",
    "df_to_kafka = df_windowed.selectExpr(\n",
    "    \"CAST(device_id AS STRING) AS key\",\n",
    "    \"CAST(to_json(struct(*)) AS STRING) AS value\"\n",
    ")\n",
    "\n",
    "# 7. Write back to Kafka\n",
    "query = df_to_kafka.writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"topic\", \"output_topic\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/kafka_window_checkpoint\") \\\n",
    "    .outputMode(\"update\") \\  # update because aggregations are used\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PySpark_Streams",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
