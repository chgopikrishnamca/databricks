{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a063ef8f-c328-4ad0-b50c-2f4726b70cdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üß† PySpark Interview Preparation Guide\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Spark Architecture & Core Concepts\n",
    "- Explain architecture of Spark Application  \n",
    "- What is lazy evaluation? What are actions and transformations?  \n",
    "- What is lineage and DAG? What is the difference among them?  \n",
    "- Explain DAG in detail  \n",
    "- What is the job of Catalyst Optimizer?  \n",
    "- What is checkpointing, why is it needed? Show me how we can do it  \n",
    "- What are memory overflow errors? When can we see this? How to handle them?  \n",
    "- What is Spark dynamic and static allocation? How can we choose?\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ Data Structures: RDDs, DataFrames, Datasets\n",
    "- What are differences among RDDs, DataFrames, and Datasets? Did you use any of them? If so, for what purposes?  \n",
    "- What are some differences among Pandas and PySpark?\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ Transformations & Actions\n",
    "- Explain differences among narrow and wide transformations  \n",
    "- What are the transformations that you have worked upon?  \n",
    "- How can we do custom transformations in PySpark?\n",
    "\n",
    "---\n",
    "\n",
    "## üßä Caching & Persistence\n",
    "- Differences between cache and persist in detail  \n",
    "- Explain how caching works by default in Spark? When to use caching techniques?\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Data Cleaning & Handling\n",
    "- How to handle duplicate values and null values? At the time of loading and after loading? For rows and columns? How do you handle missing data?  \n",
    "- What is schema inference? How to define a schema manually?  \n",
    "- How do you handle schema evolution in Spark?\n",
    "\n",
    "---\n",
    "\n",
    "## üìÇ File Formats & Storage\n",
    "- File formats known? CSV, Parquet? What is the algorithm for Parquet? Snappy  \n",
    "- What is the problem of having bulk small files in PySpark? How does it affect performance and why?\n",
    "\n",
    "---\n",
    "\n",
    "## üß± Partitioning & Bucketing\n",
    "- What is the difference between partitioning and bucketing? Which one is better and why? Is there any performance difference?  \n",
    "- Difference between coalesce() vs repartition()\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Joins & Broadcasts\n",
    "- What are different joins that can be performed in PySpark?  \n",
    "- What is broadcast join? When to use it? How does it work?  \n",
    "- What kind of joins are supported and not supported in Structured Streaming and why?\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Window Functions & Aggregations\n",
    "- What are window functions?  \n",
    "- What are different types of windows in Structured Streaming? When to use them? Give use cases\n",
    "\n",
    "---\n",
    "\n",
    "## üß® Data Skew & Spilling\n",
    "- What is data spilling in PySpark and how to handle this?  \n",
    "- What is data skewness? How can we handle data skewness? How can salting help here?\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Performance Tuning\n",
    "- How will you tune the performance of the Spark job?  \n",
    "- What are the challenges encountered with large data sets? How can we overcome them?\n",
    "\n",
    "---\n",
    "\n",
    "## üßæ Reading & Writing Data\n",
    "- How to read and write data in Spark? Show me different ways: list, CSV, Parquet  \n",
    "- How do you read streaming data from Kafka and S3?\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ Structured Streaming\n",
    "- What is Structured Streaming? How does it work internally?  \n",
    "- What are different types of output modes and when to use them?  \n",
    "- How to handle late events in PySpark?  \n",
    "- How is Structured Streaming fault tolerant in PySpark?  \n",
    "- Explain the steps involved in Structured Streaming. Consider an example like reading from Kafka topics and writing to Kafka topics. How can we achieve it? Can you write the steps involved?\n",
    "\n",
    "---\n",
    "\n",
    "## üì° Stream Processing & Alternatives\n",
    "- Why do you need stream processing? Do you know any other competitive applications? Which one is better among them you think and why?\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Broadcast Variables & Shared State\n",
    "- What are broadcast variables?\n",
    "\n",
    "---\n",
    "\n",
    "## üß® Error Handling\n",
    "- How can we handle errors and exceptions in PySpark?\n",
    "\n",
    "---\n",
    "\n",
    "## üñ•Ô∏è Deployment & Cluster Setup\n",
    "- How many ways can we deploy Spark clusters?\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Troubleshooting & Optimization Scenarios\n",
    "- Your Spark job is running out of memory with OutOfMemoryError. What steps would you take?  \n",
    "- If we have 100GB of data (with 2 files, let‚Äôs say 50GB each), and we have 50GB memory in cluster ‚Äî are we able to process the data? If so, how? Explain mechanisms.  \n",
    "- You have a 2TB dataset and Spark job is failing with OOM. How do you fix it?  \n",
    "- You have a slow running PySpark job. How can you optimize it?  \n",
    "- Streaming job is lagging behind real-time data. What could be reasons?\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ Interoperability with Pandas\n",
    "- How to convert Pandas to a DataFrame in PySpark and vice versa?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dbfb39d-df66-4880-8719-f48df7825a7d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Explain lazy evaluation in PySpark"
    }
   },
   "source": [
    "- Lazy evaluation means transformations in PySpark are not executed immediately. Instead, Spark builds a logical execution plan.\n",
    "- Actions trigger the execution, which allows optimization like pipelining and minimizing data shuffling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26537f6c-8d6c-4aa4-a44b-d4263c9f3ce7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "How does caching work in PySpark?"
    }
   },
   "outputs": [],
   "source": [
    "- Caching stores intermediate results in memory across operations, preventing recomputation. \n",
    "- Use df.cache() or df.persist() to cache DataFrames. \n",
    "- It improves performance when the same data is accessed multiple times.\n",
    "- df.persist() modes \n",
    "    - df.persist(StorageLevel.MEMORY_AND_DISK),  df.persist(StorageLevel.MEMORY_ONLY)\n",
    "- df.cache() is a synonym for df.persist(StorageLevel.MEMORY_ONLY_SER)\n",
    "- df.persist() is a synonym for df.persist(StorageLevel.MEMORY_ONLY_SER)\n",
    "- df.unpersist() if done with cache() or persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19c8cb3e-5f0e-42ea-9230-48788465a9cc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "What is the difference between wide and narrow transformations?"
    }
   },
   "outputs": [],
   "source": [
    "- Narrow Transformations: Data required for a computation comes from a single partition (e.g., map, filter).\n",
    "- Wide Transformations: Data shuffling across partitions (e.g., groupByKey, reduceByKey)."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PySpark_Q&A",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
